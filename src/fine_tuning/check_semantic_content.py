#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Check training data for Labkovsky's core ideas using SEMANTIC SIMILARITY.
FIXED: Higher threshold, better counting logic.
"""

import json
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer

# ============================================================
# CONFIG
# ============================================================

SCRIPT_DIR = Path(__file__).resolve().parent
_possible_paths = [
    SCRIPT_DIR / "data" / "fine_tuning" / "qa_rs_corpus_short.jsonl",
    Path(r"C:\Projects\projects_py\labkovsky-model\data\fine_tuning\qa_rs_corpus_short.jsonl"),
]

DATA_PATH = None
for p in _possible_paths:
    if p.exists():
        DATA_PATH = p
        break

EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# Higher threshold - 0.7+ means strong semantic match
HIGH_THRESHOLD = 0.75  # Strong match - clearly about this idea
MED_THRESHOLD = 0.65   # Moderate match - related to this idea

# ============================================================
# LABKOVSKY'S CORE IDEAS
# ============================================================

LABKOVSKY_IDEAS = {
    "self_priority": [
        "Ð’Ñ‹ ÑÐ°Ð¼Ñ‹Ð¹ Ð²Ð°Ð¶Ð½Ñ‹Ð¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð² ÑÐ²Ð¾ÐµÐ¹ Ð¶Ð¸Ð·Ð½Ð¸. Ð¡Ñ‚Ð°Ð²ÑŒÑ‚Ðµ ÑÐµÐ±Ñ Ð½Ð° Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð¼ÐµÑÑ‚Ð¾.",
        "Ð›ÑŽÐ±Ð¸Ñ‚Ðµ ÑÐµÐ±Ñ, Ð·Ð°Ð±Ð¾Ñ‚ÑŒÑ‚ÐµÑÑŒ Ð¾ ÑÐµÐ±Ðµ Ð² Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ.",
        "Ð’Ð°ÑˆÐ¸ Ð¶ÐµÐ»Ð°Ð½Ð¸Ñ Ð¸ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð½Ð¾ÑÑ‚Ð¸ Ð²Ð°Ð¶Ð½ÐµÐµ Ñ‡ÑƒÐ¶Ð¸Ñ… Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ð¹.",
    ],
    
    "leave_bad_relationships": [
        "Ð£Ñ…Ð¾Ð´Ð¸Ñ‚Ðµ Ð¾Ñ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ð°Ð¼ Ð½Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚.",
        "ÐÐµ Ð±ÐµÐ³Ð°Ð¹Ñ‚Ðµ Ð·Ð° Ñ‚ÐµÐ¼Ð¸, ÐºÑ‚Ð¾ Ð²Ð°Ñ Ð½Ðµ Ñ…Ð¾Ñ‡ÐµÑ‚.",
        "Ð Ð°ÑÑÑ‚Ð°Ð²Ð°Ð¹Ñ‚ÐµÑÑŒ Ñ Ñ‚ÐµÐ¼Ð¸, ÐºÑ‚Ð¾ Ð²Ð°Ñ Ð½Ðµ Ñ†ÐµÐ½Ð¸Ñ‚.",
        "Ð•ÑÐ»Ð¸ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½ÑÑŽÑ‚ Ð±Ð¾Ð»ÑŒ - ÑƒÑ…Ð¾Ð´Ð¸Ñ‚Ðµ.",
    ],
    
    "no_compromise": [
        "ÐšÐ¾Ð¼Ð¿Ñ€Ð¾Ð¼Ð¸ÑÑÑ‹ Ð²Ñ€ÐµÐ´ÑÑ‚, Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð±ÑƒÐ´ÐµÑ‚ Ð±Ð¾Ð»ÑŒÐ½Ð¾.",
        "ÐÐµ Ñ‚ÐµÑ€Ð¿Ð¸Ñ‚Ðµ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ð²Ð°Ð¼ Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ.",
        "Ð¢ÐµÑ€Ð¿ÐµÐ½Ð¸Ðµ Ð² Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸ÑÑ… Ñ€Ð°Ð·Ñ€ÑƒÑˆÐ°ÐµÑ‚ Ð²Ð°Ñ.",
    ],
    
    "ignore_critics": [
        "ÐÐµ Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ð¹Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð½Ð° Ñ‚ÐµÑ…, ÐºÐ¾Ð¼Ñƒ Ð²Ñ‹ Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÐµÑÑŒ.",
        "ÐŸÐ»ÐµÐ²Ð°Ñ‚ÑŒ Ð½Ð° Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ð´ÑƒÐ¼Ð°ÑŽÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¾ Ð²Ð°Ñ.",
        "ÐœÐ½ÐµÐ½Ð¸Ðµ Ð¾ÐºÑ€ÑƒÐ¶Ð°ÑŽÑ‰Ð¸Ñ… Ð½Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð²Ð»Ð¸ÑÑ‚ÑŒ Ð½Ð° Ð²Ð°ÑˆÑƒ Ð¶Ð¸Ð·Ð½ÑŒ.",
    ],
    
    "six_rules": [
        "Ð”ÐµÐ»Ð°Ð¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ñ…Ð¾Ñ‡ÐµÑ‚ÑÑ.",
        "ÐÐµ Ð´ÐµÐ»Ð°Ð¹Ñ‚Ðµ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð½Ðµ Ñ…Ð¾Ñ‡ÐµÑ‚ÑÑ.",
        "Ð¡Ñ€Ð°Ð·Ñƒ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ Ð¾ Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ.",
        "ÐÐµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ, ÐºÐ¾Ð³Ð´Ð° Ð½Ðµ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÑŽÑ‚.",
        "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ.",
        "Ð’Ñ‹ÑÑÐ½ÑÑ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ, Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾ ÑÐµÐ±Ðµ.",
    ],
    
    "say_once_then_leave": [
        "Ð¡ÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð· Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ. Ð•ÑÐ»Ð¸ Ð½Ðµ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑÑ - ÑƒÑ…Ð¾Ð´Ð¸Ñ‚Ðµ.",
        "ÐÐµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¾ÑÑŒÐ±Ñ‹. Ð¡ÐºÐ°Ð·Ð°Ð»Ð¸ Ñ€Ð°Ð· - Ð¸ Ð²ÑÑ‘.",
    ],
    
    "behavior_change": [
        "ÐœÐµÐ½ÑÐ¹Ñ‚Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ, Ð° Ð½Ðµ ÐºÐ¾Ð¿Ð°Ð¹Ñ‚ÐµÑÑŒ Ð² Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°Ñ….",
        "ÐÐµÐ²Ñ€Ð¾Ð· Ð»ÐµÑ‡Ð¸Ñ‚ÑÑ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ, Ð° Ð½Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð³Ð¾.",
    ],
}

# ============================================================
# FUNCTIONS
# ============================================================

def load_data(path):
    if path is None or not path.exists():
        print(f"âŒ File not found: {path}")
        return []
    
    records = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                if not data.get("short_answer"):
                    records.append(data)
    return records


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def analyze_similarities(records, model):
    """
    Compute similarity scores for all answers against all ideas.
    Returns detailed statistics.
    """
    print(f"ðŸ“Š Embedding {len(records)} answers...")
    
    # Embed all answers
    answers = [f"passage: {r['answer']}" for r in records]
    answer_embeddings = model.encode(answers, show_progress_bar=True)
    
    # Embed all ideas
    all_ideas = []
    idea_to_category = {}
    for category, texts in LABKOVSKY_IDEAS.items():
        for text in texts:
            all_ideas.append(f"query: {text}")
            idea_to_category[len(all_ideas)-1] = category
    
    print(f"   Embedding {len(all_ideas)} reference ideas...")
    idea_embeddings = model.encode(all_ideas)
    
    # Compute all similarities
    print(f"   Computing similarities...")
    
    # For each answer, find best matching category and its similarity
    answer_best_matches = []
    
    for ans_idx, ans_emb in enumerate(answer_embeddings):
        category_max_sims = {}
        
        for idea_idx, idea_emb in enumerate(idea_embeddings):
            cat = idea_to_category[idea_idx]
            sim = cosine_similarity(ans_emb, idea_emb)
            
            if cat not in category_max_sims or sim > category_max_sims[cat]["sim"]:
                category_max_sims[cat] = {
                    "sim": sim,
                    "idea_idx": idea_idx,
                }
        
        # Find best category for this answer
        best_cat = max(category_max_sims.keys(), key=lambda c: category_max_sims[c]["sim"])
        best_sim = category_max_sims[best_cat]["sim"]
        
        answer_best_matches.append({
            "answer_idx": ans_idx,
            "best_category": best_cat,
            "best_sim": best_sim,
            "all_categories": {c: v["sim"] for c, v in category_max_sims.items()},
            "question": records[ans_idx]["question"][:80],
            "answer": records[ans_idx]["answer"][:150],
        })
    
    return answer_best_matches


def main():
    print("=" * 70)
    print("ðŸŽ¯ LABKOVSKY SEMANTIC CONTENT CHECK (FIXED)")
    print(f"   Using: {EMBEDDING_MODEL}")
    print(f"   High threshold: {HIGH_THRESHOLD}")
    print(f"   Medium threshold: {MED_THRESHOLD}")
    print("=" * 70)
    
    # Load data
    records = load_data(DATA_PATH)
    if not records:
        return
    
    print(f"\nðŸ“‚ Loaded {len(records)} answers\n")
    
    # Load model
    print(f"ðŸ¤– Loading embedding model...")
    model = SentenceTransformer(EMBEDDING_MODEL)
    print("   Done.\n")
    
    # Analyze
    matches = analyze_similarities(records, model)
    
    # Statistics
    print("\n" + "=" * 70)
    print("ðŸ“Š SIMILARITY DISTRIBUTION")
    print("=" * 70)
    
    sims = [m["best_sim"] for m in matches]
    print(f"\n   Best similarity per answer:")
    print(f"   Min:  {min(sims):.3f}")
    print(f"   Max:  {max(sims):.3f}")
    print(f"   Mean: {np.mean(sims):.3f}")
    print(f"   Median: {np.median(sims):.3f}")
    
    # Distribution buckets
    buckets = {
        "0.80+": len([s for s in sims if s >= 0.80]),
        "0.75-0.80": len([s for s in sims if 0.75 <= s < 0.80]),
        "0.70-0.75": len([s for s in sims if 0.70 <= s < 0.75]),
        "0.65-0.70": len([s for s in sims if 0.65 <= s < 0.70]),
        "0.60-0.65": len([s for s in sims if 0.60 <= s < 0.65]),
        "<0.60": len([s for s in sims if s < 0.60]),
    }
    
    print(f"\n   Distribution:")
    for bucket, count in buckets.items():
        pct = 100 * count / len(sims)
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {bucket:10} {bar:25} {count:3} ({pct:.1f}%)")
    
    # By category (high threshold)
    print("\n" + "=" * 70)
    print(f"ðŸ“‹ COVERAGE BY CATEGORY (threshold >= {HIGH_THRESHOLD})")
    print("=" * 70)
    
    category_counts = {cat: {"high": 0, "med": 0} for cat in LABKOVSKY_IDEAS.keys()}
    
    for m in matches:
        for cat, sim in m["all_categories"].items():
            if sim >= HIGH_THRESHOLD:
                category_counts[cat]["high"] += 1
            elif sim >= MED_THRESHOLD:
                category_counts[cat]["med"] += 1
    
    for cat in LABKOVSKY_IDEAS.keys():
        high = category_counts[cat]["high"]
        med = category_counts[cat]["med"]
        high_pct = 100 * high / len(records)
        med_pct = 100 * med / len(records)
        print(f"\n   {cat}:")
        print(f"      Strong (>={HIGH_THRESHOLD}): {high:3} ({high_pct:.1f}%)")
        print(f"      Moderate (>={MED_THRESHOLD}): {med:3} ({med_pct:.1f}%)")
    
    # Top examples
    print("\n" + "=" * 70)
    print("ðŸ“ TOP MATCHING ANSWERS (highest similarity)")
    print("=" * 70)
    
    top_matches = sorted(matches, key=lambda x: -x["best_sim"])[:5]
    
    for i, m in enumerate(top_matches, 1):
        print(f"\n{i}. Similarity: {m['best_sim']:.3f} | Category: {m['best_category']}")
        print(f"   Q: {m['question']}...")
        print(f"   A: {m['answer']}...")
    
    # Lowest examples
    print("\n" + "=" * 70)
    print("ðŸ“ LOWEST MATCHING ANSWERS (generic content?)")
    print("=" * 70)
    
    low_matches = sorted(matches, key=lambda x: x["best_sim"])[:5]
    
    for i, m in enumerate(low_matches, 1):
        print(f"\n{i}. Similarity: {m['best_sim']:.3f} | Category: {m['best_category']}")
        print(f"   Q: {m['question']}...")
        print(f"   A: {m['answer']}...")
    
    # Final assessment
    print("\n" + "=" * 70)
    print("ðŸ“‹ ASSESSMENT")
    print("=" * 70)
    
    high_count = len([s for s in sims if s >= HIGH_THRESHOLD])
    med_count = len([s for s in sims if s >= MED_THRESHOLD])
    
    high_pct = 100 * high_count / len(records)
    med_pct = 100 * med_count / len(records)
    
    print(f"\n   Strong matches (>={HIGH_THRESHOLD}): {high_count}/{len(records)} ({high_pct:.1f}%)")
    print(f"   Moderate+ matches (>={MED_THRESHOLD}): {med_count}/{len(records)} ({med_pct:.1f}%)")
    
    if high_pct < 20:
        print(f"\n   âš ï¸ Low strong coverage. Most answers may be too generic.")
    elif high_pct < 40:
        print(f"\n   â„¹ï¸ Moderate coverage. Some distinctive content present.")
    else:
        print(f"\n   âœ… Good coverage. Data contains distinctive Labkovsky ideas.")


if __name__ == "__main__":
    main()