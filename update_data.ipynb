{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5de0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "sys.path.append(str(project_root / 'src' / 'data_processing'))\n",
    "sys.path.append(str(project_root / 'src' / 'fine_tuning'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f60c8",
   "metadata": {},
   "source": [
    "Simplified ID Strategy\\n\n",
    "Source|Current|NewID|Example\n",
    "QA  no id {video_id}_{order} 5R0FIkU97HY_01\n",
    "Interview   has chunk_id    already unique  int_{interview_id}_17\n",
    "Article     has chunk_id    already unique  art_{article_id}_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d750d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_qa_file(input_path: str, output_path: str):\n",
    "    \"\"\"Add order-based IDs to QA pairs\"\"\"\n",
    "    \n",
    "    items = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                items.append(json.loads(line))\n",
    "    \n",
    "    # Count per video\n",
    "    video_counter = defaultdict(int)\n",
    "    \n",
    "    for item in items:\n",
    "        video_id = item['video_id']\n",
    "        video_counter[video_id] += 1\n",
    "        item['id'] = f\"{video_id}_{video_counter[video_id]:02d}\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in items:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"‚úì QA: {len(items)} items, {len(video_counter)} videos\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ffe67",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e99b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 10\n",
      "1: 1320 chars (~440 tokens)\n",
      "2: 3632 chars (~1210 tokens)\n",
      "3: 1786 chars (~595 tokens)\n",
      "4: 2405 chars (~801 tokens)\n",
      "5: 4532 chars (~1510 tokens)\n",
      "6: 3583 chars (~1194 tokens)\n",
      "7: 1235 chars (~411 tokens)\n",
      "8: 4309 chars (~1436 tokens)\n",
      "9: 2235 chars (~745 tokens)\n",
      "10: 2359 chars (~786 tokens)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "lines = open('./data/fine_tuning/train_data.jsonl', encoding='utf-8').readlines()\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip():\n",
    "        r = json.loads(line)\n",
    "        chars = len(r['input'] + r['output'])\n",
    "        print(f\"{i+1}: {chars} chars (~{chars//3} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1796c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Article 1 ===\n",
      "Questions: ['–ß—Ç–æ –∑–Ω–∞—á–∏—Ç –±—ã—Ç—å –∂–µ–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏ –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è —Å–∏–ª—å–Ω–æ–π –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –∂–µ–Ω—â–∏–Ω–æ–π?', '–ö–∞–∫ –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –∂–µ–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å, –∫–æ–≥–¥–∞ —Ö–æ—á–µ—à—å –±—ã—Ç—å —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ–π –≤ —Å–µ–±–µ?', '–ü–æ—á–µ–º—É –æ–±—Ä–∞–∑ —É—Å–ø–µ—à–Ω–æ–π –∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π –∂–µ–Ω—â–∏–Ω—ã –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –∫–∞–∫ ¬´–º—É–∂–∏–∫ –≤ —é–±–∫–µ¬ª?']\n",
      "Text preview: \"–ë—ã—Ç—å –∂–µ–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –Ω–µ –∑–Ω–∞—á–∏—Ç –±—ã—Ç—å —Ç—Ä—è–ø–∫–æ–π\" –ö–æ–≥–¥–∞ —è –≥–æ–≤–æ—Ä—é –æ —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ–π –≤ —Å–µ–±–µ –∂–µ–Ω—â–∏–Ω–µ, –º–Ω–æ–≥–∏–µ –≤–æ–∑—Ä–∞–∂–∞—é—Ç: –≤–µ–¥—å –ø–æ–ª—É—á–∏—Ç—Å—è –º—É–∂–∏–∫ –≤ —é–±–∫–µ! –î–∞–≤–∞–π—Ç–µ –æ–± —ç—Ç–æ–º –ø–æ–≥–æ–≤–æ—Ä–∏–º. –£ –º–Ω–æ–≥–∏—Ö –∂–µ–Ω—â–∏–Ω, —á–∏—Ç–∞—é—â–∏—Ö –º–æ–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Å–ª—É—à–∞—é—â–∏—Ö –ª–µ–∫—Ü–∏–∏, –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–æ—Ç–µ—Å—Ç. –ö–æ–≥–¥–∞ —è –≥–æ–≤–æ—Ä—é –æ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ...\n",
      "\n",
      "=== Article 2 ===\n",
      "Questions: ['–ü–æ—á–µ–º—É –º—É–∂—á–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–∑–¥—Ä–∞–∂–∞—Ç—å—Å—è –∏–∑-–∑–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø—Ä–æ—Å—å–± –∂–µ–Ω—â–∏–Ω—ã?', '–ö–∞–∫ –¥–µ–Ω—å–≥–∏ –≤–ª–∏—è—é—Ç –Ω–∞ –≤–ª–∞—Å—Ç—å –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤ —Å–µ–º—å–µ –º–µ–∂–¥—É –º—É–∂—á–∏–Ω–æ–π –∏ –∂–µ–Ω—â–∏–Ω–æ–π?', '–ü–æ—á–µ–º—É —Å –≤–æ–∑—Ä–∞—Å—Ç–æ–º —É –∂–µ–Ω—â–∏–Ω—ã —Å–Ω–∏–∂–∞–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–µ—Å –º—É–∂—á–∏–Ω—ã, –µ—Å–ª–∏ —É –Ω–µ–µ –Ω–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –∂–∏–∑–Ω–∏?']\n",
      "Text preview: –ñ–µ–Ω—â–∏–Ω–∞ –∫–∞–∫ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Ä–∞—Å—Ö–æ–¥–æ–≤ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–∑–¥—Ä–∞–∂–∞—Ç—å. –ï–µ –Ω—É–∂–¥—ã –∫–∞–∂—É—Ç—Å—è –Ω–µ —Å—Ç–æ–ª—å —É–∂ –≤–∞–∂–Ω—ã–º–∏, –µ–µ –ø—Ä–æ—Å—å–±—ã –≤ –≥–æ–ª–æ–≤–µ –º—É–∂—á–∏–Ω—ã –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ ¬´–ø–∏–ª–µ–∂–∫—É¬ª –∏ –Ω—ã—Ç—å–µ, –æ–Ω —É–∂–µ –Ω–µ —Ö–æ—á–µ—Ç –µ–µ –ø–æ—Ä–∞–¥–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–∫–æ–π –∏–ª–∏ –ø–æ–¥–∞—Ä–∫–æ–º, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –≤—Å–µ –≤—Ä–µ–º—è –ø—Ä–æ—Å–∏—Ç. –ò –ø—Ä–æ—Å–∏—Ç, –∏ –ø—Ä–æ—Å–∏—Ç, –Ω—É —Å–∫–æ–ª—å–∫–æ –º–æ–∂–Ω–æ. –î–µ—Ç...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Check article format\n",
    "with open('./data/processed/articles_with_questions.jsonl', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if line.strip() and i < 2:\n",
    "            r = json.loads(line)\n",
    "            print(f\"=== Article {i+1} ===\")\n",
    "            print(f\"Questions: {r.get('potential_questions', [])}\")\n",
    "            print(f\"Text preview: {r['text'][:300]}...\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b98b3d",
   "metadata": {},
   "source": [
    "prepare qa_pairs to train and test data for ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d5e405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing train examples: 10\n",
      "QA pairs: 241\n",
      "Total train: 226\n",
      "Test: 25\n",
      "‚úÖ Saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load existing train_data (your 10 examples)\n",
    "train_data = []\n",
    "with open('./data/fine_tuning/train_data.jsonl', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            train_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Existing train examples: {len(train_data)}\")\n",
    "\n",
    "# Load qa_pairs\n",
    "qa_pairs = []\n",
    "with open('./data/processed/qa_pairs.jsonl', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            r = json.loads(line)\n",
    "            qa_pairs.append({\n",
    "                \"input\": r['question'],\n",
    "                \"output\": r['answer']\n",
    "            })\n",
    "\n",
    "print(f\"QA pairs: {len(qa_pairs)}\")\n",
    "\n",
    "# Split qa_pairs: 90% train, 10% test\n",
    "random.seed(42)\n",
    "random.shuffle(qa_pairs)\n",
    "split = int(len(qa_pairs) * 0.9)\n",
    "\n",
    "# Combine\n",
    "train_data.extend(qa_pairs[:split])\n",
    "test_data = qa_pairs[split:]\n",
    "\n",
    "print(f\"Total train: {len(train_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "# Save\n",
    "with open('./data/fine_tuning/train_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('./data/fine_tuning/test_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"‚úÖ Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12901b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles added: 250\n",
      "Total train examples: 476\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Add articles - only FIRST question per chunk\n",
    "articles_added = 0\n",
    "with open('./data/processed/articles_with_questions.jsonl', encoding='utf-8') as f:\n",
    "    articles = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "with open('./data/fine_tuning/train_data.jsonl', 'a', encoding='utf-8') as f:\n",
    "    for r in articles:\n",
    "        questions = r.get('potential_questions', [])\n",
    "        if questions:\n",
    "            item = {\n",
    "                \"input\": questions[0],  # Only first question\n",
    "                \"output\": r['text']\n",
    "            }\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "            articles_added += 1\n",
    "\n",
    "print(f\"Articles added: {articles_added}\")\n",
    "\n",
    "# Count total\n",
    "with open('./data/fine_tuning/train_data.jsonl', encoding='utf-8') as f:\n",
    "    total = len([l for l in f if l.strip()])\n",
    "print(f\"Total train examples: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b011da4",
   "metadata": {},
   "source": [
    "working on interview_ft.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc906f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded interviews: 18\n",
      "Total train examples: 494\n",
      "‚úÖ Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read entire file as one JSON or multiple JSON objects\n",
    "with open('./data/processed/intervie_ft.jsonl', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Try to parse - might be array or single objects separated by whitespace\n",
    "interviews = []\n",
    "decoder = json.JSONDecoder()\n",
    "content = content.strip()\n",
    "pos = 0\n",
    "\n",
    "while pos < len(content):\n",
    "    try:\n",
    "        obj, end = decoder.raw_decode(content, pos)\n",
    "        interviews.append(obj)\n",
    "        pos = end\n",
    "        # Skip whitespace\n",
    "        while pos < len(content) and content[pos] in ' \\n\\t\\r':\n",
    "            pos += 1\n",
    "    except json.JSONDecodeError:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded interviews: {len(interviews)}\")\n",
    "\n",
    "# Add to train_data\n",
    "with open('./data/fine_tuning/train_data.jsonl', 'a', encoding='utf-8') as f:\n",
    "    for r in interviews:\n",
    "        messages = r.get('messages', [])\n",
    "        user_msg = None\n",
    "        assistant_msg = None\n",
    "        for m in messages:\n",
    "            if m['role'] == 'user':\n",
    "                user_msg = m['content']\n",
    "            elif m['role'] == 'assistant':\n",
    "                assistant_msg = m['content']\n",
    "        \n",
    "        if user_msg and assistant_msg:\n",
    "            item = {\n",
    "                \"input\": user_msg,\n",
    "                \"output\": assistant_msg\n",
    "            }\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Count total\n",
    "with open('./data/fine_tuning/train_data.jsonl', encoding='utf-8') as f:\n",
    "    total = len([l for l in f if l.strip()])\n",
    "print(f\"Total train examples: {total}\")\n",
    "print(\"‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fc02a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answers (<500 chars): 127\n",
      "Long answers (>2000 chars): 21\n",
      "Likely Q&A: 473\n",
      "Likely articles: 21\n"
     ]
    }
   ],
   "source": [
    "# Check data balance\n",
    "import json\n",
    "\n",
    "qa_count = 0\n",
    "article_count = 0\n",
    "short_answers = 0\n",
    "long_answers = 0\n",
    "\n",
    "with open('./data/fine_tuning/train_data.jsonl', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            r = json.loads(line)\n",
    "            out_len = len(r['output'])\n",
    "            \n",
    "            if out_len > 2000:\n",
    "                article_count += 1\n",
    "                long_answers += 1\n",
    "            else:\n",
    "                qa_count += 1\n",
    "            \n",
    "            if out_len < 500:\n",
    "                short_answers += 1\n",
    "\n",
    "print(f\"Short answers (<500 chars): {short_answers}\")\n",
    "print(f\"Long answers (>2000 chars): {long_answers}\")\n",
    "print(f\"Likely Q&A: {qa_count}\")\n",
    "print(f\"Likely articles: {article_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e995db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 250\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Append qa_pairs to existing 10\n",
    "with open('./data/processed/qa_pairs.jsonl', encoding='utf-8') as f:\n",
    "    qa_pairs = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "with open('./data/fine_tuning/train_data.jsonl', 'a', encoding='utf-8') as f:\n",
    "    for r in qa_pairs:\n",
    "        item = {\"input\": r['question'], \"output\": r['answer']}\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Count total\n",
    "with open('./data/fine_tuning/train_data.jsonl', encoding='utf-8') as f:\n",
    "    total = len([l for l in f if l.strip()])\n",
    "    \n",
    "print(f\"Total: {total}\")  # Should be 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b5e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kept: 245 examples\n",
      "‚ùå Removed: 6 examples\n",
      "\n",
      "Removed examples:\n",
      "  [3 tok] Q: –ö–∞–∫ –∂–∏—Ç—å —Å –º—É–∂–µ–º-–Ω–µ–≤—Ä–æ—Ç–∏–∫–æ–º –∏ –±—ã—Ç—å —Å—á–∞—Å—Ç–ª–∏–≤–æ–π?...\n",
      "           A: –ù–∏–∫–∞–∫.\n",
      "\n",
      "  [9 tok] Q: –ù–µ —Ö–æ—á–µ—Ç—Å—è –≤—ã—Ö–æ–¥–∏—Ç—å –∏–∑ –¥–æ–º–∞. –≠—Ç–æ —Å–æ—Ü–∏–æ—Ñ–æ–±–∏—è?...\n",
      "           A: –ù–µ—Ç, —ç—Ç–æ –¥–µ–ø—Ä–µ—Å—Å–∏—è.\n",
      "\n",
      "  [12 tok] Q: –£ –º–µ–Ω—è –µ—Å—Ç—å –º—É–∂ –∏ –µ—Å—Ç—å —Ä–æ–¥–∏—Ç–µ–ª–∏. –Ø –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω...\n",
      "           A: –ú–æ–∂–µ—Ç, –Ω–æ –Ω–µ –¥–æ–ª–∂–µ–Ω, —è —Ç–∞–∫ —Å—á–∏—Ç–∞—é.\n",
      "\n",
      "  [28 tok] Q: –ú–Ω–µ 20 –ª–µ—Ç. –Ø –∂–∏–ª–∞ —Å –±–∞–±—É—à–∫–æ–π –∏ –¥–µ–¥—É—à–∫–æ–π, —Å –º–∞–º–æ–π –∏ –ø–∞–ø–æ–π –æ–±...\n",
      "           A: –ê –≤ —à–∫–æ–ª–µ –≤–ª—é–±–ª—è–ª–∏—Å—å –≤ –∫–æ–≥–æ-–Ω–∏–±—É–¥—å? –ï—Å–ª–∏ –¥–∞ ‚Äî –±—É–¥–µ—Ç–µ –∂–∏—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ.\n",
      "\n",
      "  [37 tok] Q: –ï—Å—Ç—å –ª–∏ —Ç–∞–∫–æ–µ –ø–æ–Ω—è—Ç–∏–µ, –∫–∞–∫ –∑–¥–æ—Ä–æ–≤–∞—è –∏ –Ω–µ–∑–¥–æ—Ä–æ–≤–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –≤...\n",
      "           A: –ù–µ—Ç –ø—Ä–æ–±–ª–µ–º. –≠—Ç–æ –¥–µ–ª–æ –∫–∞–∂–¥–æ–≥–æ. –Ø –≤–æ–æ–±—â–µ –≤ –∂–µ–Ω—Å–∫–æ–º –≤–æ–∑—Ä–∞—Å—Ç–µ –Ω–µ –ø–æ–Ω–∏–º–∞—é ‚Äî –º–Ω–µ —á—Ç–æ 60, —á—Ç–æ 25, –æ–¥–Ω–æ –∏ —Ç\n",
      "\n",
      "  [46 tok] Q: –ü–æ—Å–ª–µ —Å–º–µ—Ä—Ç–∏ —Ç—ë—Ç–∏, —Å –∫–æ—Ç–æ—Ä–æ–π —è –±—ã–ª–∞ –æ—á–µ–Ω—å –±–ª–∏–∑–∫–∞, –º–Ω–µ –æ—á–µ–Ω—å ...\n",
      "           A: –í–∞—à–∞ —Ç—ë—Ç—è –±—ã–ª–∞ –±—ã —Ä–∞–¥–∞, –µ—Å–ª–∏ –±—ã –≤—ã –ø–µ—Ä–µ—Å—Ç–∞–ª–∏ –≥–æ—Ä–µ–≤–∞—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å–ª–∏ –±—ã –æ–Ω–∞ –±—ã–ª–∞ –∂–∏–≤–∞, –æ–Ω–∞ –±—ã –æ—á–µ–Ω\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "data_path = Path(\"data/fine_tuning/train_data.jsonl\")\n",
    "output_path = Path(\"data/fine_tuning/train_data_clean.jsonl\")\n",
    "\n",
    "MIN_ANSWER_TOKENS = 50  # Minimum for a \"real\" answer with substance\n",
    "\n",
    "kept = []\n",
    "removed = []\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        answer_tokens = len(tokenizer.encode(r['output']))\n",
    "        \n",
    "        if answer_tokens >= MIN_ANSWER_TOKENS:\n",
    "            kept.append(r)\n",
    "        else:\n",
    "            removed.append({\n",
    "                'tokens': answer_tokens,\n",
    "                'q': r['input'][:60],\n",
    "                'a': r['output'][:100]\n",
    "            })\n",
    "\n",
    "# Save cleaned data\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for r in kept:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Report\n",
    "print(f\"‚úÖ Kept: {len(kept)} examples\")\n",
    "print(f\"‚ùå Removed: {len(removed)} examples\\n\")\n",
    "\n",
    "print(\"Removed examples:\")\n",
    "for ex in sorted(removed, key=lambda x: x['tokens']):\n",
    "    print(f\"  [{ex['tokens']} tok] Q: {ex['q']}...\")\n",
    "    print(f\"           A: {ex['a']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f12d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready\n",
      "[\n",
      "    \"–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–≤—Ä–æ–∑ –ø–æ –≤–∞—à–µ–º—É –º–Ω–µ–Ω–∏—é?\",\n",
      "    \"–ö–∞–∫–∏–µ –∏–∑ —à–µ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª –≤–∞–º —Å–ª–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ –ø–æ—á–µ–º—É?\",\n",
      "    \"–ú–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ —à–µ—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ?\"\n"
     ]
    }
   ],
   "source": [
    "from inference_lora import load_model, ask\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "prompt = \"\"\"Generate 2-3 questions (in Russian) for this text:\n",
    "\n",
    "–°–æ–≤–µ—Ç ¬´–¥–µ–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ö–æ—á–µ—Ç—Å—è¬ª –Ω–∞—à–∏ –≥—Ä–∞–∂–¥–∞–Ω–µ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—é—Ç –∫–∞–∫ –ø—Ä–∏–∑—ã–≤ –∫ –∞–Ω–∞—Ä—Ö–∏–∏. –°–≤–æ–∏ —Å–∞–º—ã–µ —Å–∏–ª—å–Ω—ã–µ –∂–µ–ª–∞–Ω–∏—è –æ–Ω–∏ —Å—á–∏—Ç–∞—é—Ç –Ω–µ–ø—Ä–µ–º–µ–Ω–Ω–æ –Ω–∏–∑–º–µ–Ω–Ω—ã–º–∏, –ø–æ—Ä–æ—á–Ω—ã–º–∏, –æ–ø–∞—Å–Ω—ã–º–∏ –¥–ª—è –æ–∫—Ä—É–∂–∞—é—â–∏—Ö. –õ—é–¥–∏ —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –æ–Ω–∏ —Ç–∞–π–Ω—ã–µ –±–µ—Å–ø—Ä–µ–¥–µ–ª—å—â–∏–∫–∏, –∏ –ø–æ–ø—Ä–æ—Å—Ç—É –±–æ—è—Ç—Å—è –¥–∞—Ç—å —Å–µ–±–µ –≤–æ–ª—é! –Ø –≤–∏–∂—É –≤ —ç—Ç–æ–º —Å–µ—Ä—å–µ–∑–Ω—ã–π —Å–∏–º–ø—Ç–æ–º –≤—Å–µ–æ–±—â–µ–≥–æ –Ω–µ–≤—Ä–æ–∑–∞.\\n\\n–ì–æ–≤–æ—Ä–∏—à—å —á–µ–ª–æ–≤–µ–∫—É: ¬´–î–µ–ª–∞–π —Ç–æ, —á—Ç–æ —Ö–æ—á–µ—à—å!¬ª –ê –æ–Ω: ¬´–ù—É —á—Ç–æ –≤—ã! –†–∞–∑–≤–µ —Ç–∞–∫ –º–æ–∂–Ω–æ?!¬ª\\n\\n–û—Ç–≤–µ—á–∞—é: ¬´–ï—Å–ª–∏ –≤—ã —Å—á–∏—Ç–∞–µ—Ç–µ —Å–µ–±—è —Ö–æ—Ä–æ—à–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º, —Ç–æ –¥–∞. –ú–æ–∂–Ω–æ –∏ –Ω—É–∂–Ω–æ¬ª. –ñ–µ–ª–∞–Ω–∏—è —Ö–æ—Ä–æ—à–µ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö.\\n\\n–®–µ—Å—Ç—å –ø—Ä–∞–≤–∏–ª, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–ª–∏ –Ω–µ –æ–¥–Ω–æ–º—É –¥–µ—Å—è—Ç–∫—É –ª—é–¥–µ–π –≤—ã–π—Ç–∏ –∏–∑ –Ω–µ–≤—Ä–æ–∑–∞, ‚Äì —Ä–µ–∑—É–ª—å—Ç–∞—Ç 30 –ª–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫–∏. –≠—Ç–æ –Ω–µ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —è –¥—É–º–∞–ª –Ω–∞–¥ –Ω–∏–º–∏ 30 –ª–µ—Ç. –°–∫–æ—Ä–µ–µ, –æ–¥–Ω–∞–∂–¥—ã –æ–Ω–∏ —Å–∞–º–∏ —Å—Ç–∏—Ö–∏–π–Ω–æ –≤—ã—Å—Ç—Ä–æ–∏–ª–∏—Å—å, –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ –ú–µ–Ω–¥–µ–ª–µ–µ–≤–∞ –≤ –≥–æ–ª–æ–≤–µ –ú–µ–Ω–¥–µ–ª–µ–µ–≤–∞, –∫–æ–≥–¥–∞ –æ–Ω –ø—Ä–æ—Å–Ω—É–ª—Å—è.\\n\\n–ü—Ä–∞–≤–∏–ª–∞ –Ω–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –ø—Ä–æ—Å—Ç—ã–µ:\\n\\n1. –î–µ–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ö–æ—á–µ—Ç—Å—è.\\n\\n2. –ù–µ –¥–µ–ª–∞—Ç—å —Ç–æ–≥–æ, —á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è.\\n\\n3. –°—Ä–∞–∑—É –≥–æ–≤–æ—Ä–∏—Ç—å –æ —Ç–æ–º, —á—Ç–æ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è.\\n\\n4. –ù–µ –æ—Ç–≤–µ—á–∞—Ç—å, –∫–æ–≥–¥–∞ –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç.\\n\\n5. –û—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å.\\n\\n6. –í—ã—è—Å–Ω—è—è –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –≥–æ–≤–æ—Ä–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ —Å–µ–±–µ.\n",
    "\n",
    "Response - JSON array ONLY:\n",
    "[\"–≤–æ–ø—Ä–æ—Å 1\", \"–≤–æ–ø—Ä–æ—Å 2\", \"–≤–æ–ø—Ä–æ—Å 3\"]\"\"\"\n",
    "\n",
    "response = ask(model, tokenizer, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4348ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A: 245\n",
      "Dialogues: 44\n",
      "Total: 289\n",
      "Saved to C:\\Projects\\projects_py\\labkovsky-model\\data\\fine_tuning\\train_data_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(r\"C:\\Projects\\projects_py\\labkovsky-model\\data\\fine_tuning\")\n",
    "\n",
    "# Input files\n",
    "QA_FILE = DATA_DIR / \"train_data_clean.jsonl\"\n",
    "DIALOGUES_FILE = Path(r\"C:\\Projects\\projects_py\\labkovsky-model\\src\\fine_tuning\\data\\Hochu_i_budu_dialogues.jsonl\")\n",
    "\n",
    "# Output\n",
    "OUTPUT_FILE = DATA_DIR / \"train_data_final.jsonl\"\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# 1. Q&A ‚Üí messages\n",
    "with open(QA_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            r = json.loads(line)\n",
    "            all_records.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": r[\"input\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": r[\"output\"]}\n",
    "                ]\n",
    "            })\n",
    "\n",
    "print(f\"Q&A: {len(all_records)}\")\n",
    "\n",
    "# 2. Dialogues ‚Üí messages (rename turns to messages)\n",
    "count = 0\n",
    "with open(DIALOGUES_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            r = json.loads(line)\n",
    "            all_records.append({\n",
    "                \"messages\": r[\"turns\"]\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "print(f\"Dialogues: {count}\")\n",
    "print(f\"Total: {len(all_records)}\")\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for r in all_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9deec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DECISION TYPES ===\n",
      "SELF_ESTEEM_CORRECTIVE: 81\n",
      "EXPLANATION: 65\n",
      "DEPENDENCY_BOUNDARIES: 42\n",
      "ANXIETY_MANAGEMENT: 20\n",
      "ADDICTION_PATTERN: 14\n",
      "CLINICAL_ESCALATION: 13\n",
      "AFFECTIVE_ADDICTION: 12\n",
      "PARENTING_MODEL: 12\n",
      "FEAR_SCENARIO_COPING: 11\n",
      "PARENTING_LIMITS: 11\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def collect_decision_types(jsonl_path: str):\n",
    "    decision_types = []\n",
    "    tags_by_dt = defaultdict(list)  # dt -> list of all tags\n",
    "    \n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] Line {line_num}: invalid JSON ({e})\")\n",
    "                continue\n",
    "            \n",
    "            dt = obj.get(\"decision_type\")\n",
    "            aux_tags = obj.get(\"aux_tags\", [])\n",
    "            \n",
    "            if dt is not None:\n",
    "                decision_types.append(dt)\n",
    "                tags_by_dt[dt].extend(aux_tags)\n",
    "    \n",
    "    # Count decision types\n",
    "    dt_counter = Counter(decision_types)\n",
    "    \n",
    "    print(\"\\n=== DECISION TYPES ===\")\n",
    "    for dt, count in dt_counter.most_common():\n",
    "        print(f\"{dt}: {count}\")\n",
    "\n",
    "    \n",
    "    return dt_counter, tags_by_dt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collect_decision_types(r\".\\data\\fine_tuning\\qa_dt_corpus.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a738ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TIER COUNTS ===\n",
      "Only RAG: 12\n",
      "Short Answer: 7\n",
      "ALL: 467\n",
      "Good for FT: 448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def count_ft(jsonl_path: str):\n",
    "    # count how many have short_answer (FT) and only RAG (tier)  and how many all cases we have and how many we have that good for FT that is all - short - rag\n",
    "    countShort = 0\n",
    "    countOnlyRAG = 0\n",
    "    countALL = 0\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] Line {line_num}: invalid JSON ({e})\")\n",
    "                continue\n",
    "            \n",
    "            rag_only = obj.get(\"tier\")\n",
    "            short_answer = obj.get(\"short_answer\")\n",
    "                        \n",
    "            if rag_only is not None:\n",
    "                countOnlyRAG += 1\n",
    "            if short_answer is not None:\n",
    "                countShort += 1            \n",
    "            countALL += 1\n",
    "\n",
    "    print(\"\\n=== TIER COUNTS ===\")\n",
    "    print(f\"Only RAG: {countOnlyRAG}\")\n",
    "    print(f\"Short Answer: {countShort}\")\n",
    "    print(f\"ALL: {countALL}\")\n",
    "    print(f\"Good for FT: {countALL - countShort - countOnlyRAG}\")\n",
    "\n",
    "    \n",
    "    return countShort, countOnlyRAG\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count_ft(r\".\\data\\fine_tuning\\qa_rs_corpus_short.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451f9c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__yreUuBY_g_03_interv: 113 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "WVFDJxAyqs8_03_interv: 107 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "WVFDJxAyqs8_06_interv: 105 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "6wK5Roh_tvY_02_interv: 108 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "0LHoKduq8XI_02_interv: 109 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "0LHoKduq8XI_03_interv: 104 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "-aCsNAR0w38_01_interv: 106 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "aLQt7RU63gk_05_interv: 116 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "W5dINgfexk8_05_interv: 124 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "XrtG6NyMeU4_10_interv: 108 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "XrtG6NyMeU4_15_interv: 118 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_05_interv: 124 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_07_interv: 111 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_08_interv: 117 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_09_interv: 163 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_10_interv: 130 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_13_interv: 114 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_14_interv: 130 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "QR-gHOvk3WM_15_interv: 110 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "fKBFvwFjUks_02_interv: 129 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "fKBFvwFjUks_04_interv: 157 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "fKBFvwFjUks_08_interv: 131 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "fKBFvwFjUks_10_interv: 122 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "5R0FIkU97HY_01_interv: 104 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "5R0FIkU97HY_03_interv: 131 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "tg_2025_01_conflict_01_interv: 105 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "tg_2025_01_paying_dates_01_interv: 123 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "tg_2025_01_lazy_partner_01_interv: 145 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n",
      "tg_2025_01_difficult_period_01_interv: 163 —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('C:/Projects/projects_py/labkovsky-model/data/fine_tuning/qa_corpus_rag_optimized.jsonl', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            if item.get('chunk_role') == 'INTERVENTION':\n",
    "                words = len(item['answer'].split())\n",
    "                if words > 100:\n",
    "                    print(f\"{item['id']}: {words} —Å–ª–æ–≤ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfe5fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srJvn19GKNA EXPLANATION\n",
      "srJvn19GKNA ESCALATION\n",
      "srJvn19GKNA INTERVENTION\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"labkovsky\")\n",
    "\n",
    "# Check a sample\n",
    "sample = collection.get(limit=3, include=[\"metadatas\"])\n",
    "for meta in sample[\"metadatas\"]:\n",
    "    print(meta.get(\"video_id\"), meta.get(\"chunk_role\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0e71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\projects_py\\labkovsky-model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Projects\\projects_py\\labkovsky-model\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--Vikhrmodels--Vikhr-YandexGPT-5-Lite-8B-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "Test</s>\n",
      "<s>documents\n",
      "[{\"doc_id\": 0, \"content\": \"test\"}]</s>\n",
      "<s>user\n",
      "Question</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Test\"},\n",
    "    {\"role\": \"documents\", \"content\": '[{\"doc_id\": 0, \"content\": \"test\"}]'},\n",
    "    {\"role\": \"user\", \"content\": \"Question\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6534f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:47<00:00, 56.89s/it] \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.27s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "–¢—ã ‚Äî –ú–∏—Ö–∞–∏–ª –õ–∞–±–∫–æ–≤—Å–∫–∏–π. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø—Ä—è–º–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
      "documents\n",
      "[{\"doc_id\": 0, \"title\": \"EXPLANATION\", \"content\": \"–í –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤–∞—à–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ª–µ–∂–∏—Ç —Ç—Ä–µ–≤–æ–≥–∞. –¢–µ–ª–µ—Ñ–æ–Ω, —Ç—Ä–∞—Ç—ã, –∞–ª–∫–æ–≥–æ–ª—å –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ ‚Äî —ç—Ç–æ —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–±–∏—Ç—å –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\"}, {\"doc_id\": 1, \"title\": \"INTERVENTION\", \"content\": \"–ü—Ä–∏–∑–Ω–∞–π—Ç–µ, —á—Ç–æ –Ω–µ –º–æ–∂–µ—Ç–µ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å–∞–º–∏. –°–∫–∞–∂–∏—Ç–µ —Å–µ–±–µ: —è –Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é —ç—Ç–æ. –ü–æ—Ç–æ–º —Ä–µ—à–∏—Ç–µ ‚Äî –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ç–∞–∫ –∂–∏—Ç—å.\"}]\n",
      "user\n",
      "–£ –º–µ–Ω—è 4 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: —Ç—Ä–∞—Ç—ã, –∞–ª–∫–æ–≥–æ–ª—å, –∂–µ–Ω–∞—Ç—ã–π –º—É–∂—á–∏–Ω–∞, —Ç–µ–ª–µ—Ñ–æ–Ω. –° —á–µ–≥–æ –Ω–∞—á–∞—Ç—å?\n",
      "assistant\n",
      "–ù–∞—á–∞—Ç—å —Å—Ç–æ–∏—Ç —Å –ø—Ä–∏–∑–Ω–∞–Ω–∏—è, —á—Ç–æ –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ü—Ä–∏–∑–Ω–∞–π—Ç–µ, —á—Ç–æ –≤—ã –Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é. –ó–∞—Ç–µ–º –ø—Ä–∏–º–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ: –ª–∏–±–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∑–∞ –ø–æ–º–æ—â—å—é –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É, –ª–∏–±–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –∂–∏—Ç—å —Ç–∞–∫, –∫–∞–∫ –µ—Å—Ç—å.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Load model with 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_name = \"Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Your RAG documents\n",
    "documents = [\n",
    "    {\"doc_id\": 0, \"title\": \"EXPLANATION\", \"content\": \"–í –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –≤–∞—à–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ª–µ–∂–∏—Ç —Ç—Ä–µ–≤–æ–≥–∞. –¢–µ–ª–µ—Ñ–æ–Ω, —Ç—Ä–∞—Ç—ã, –∞–ª–∫–æ–≥–æ–ª—å –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ ‚Äî —ç—Ç–æ —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–±–∏—Ç—å –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\"},\n",
    "    {\"doc_id\": 1, \"title\": \"INTERVENTION\", \"content\": \"–ü—Ä–∏–∑–Ω–∞–π—Ç–µ, —á—Ç–æ –Ω–µ –º–æ–∂–µ—Ç–µ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å–∞–º–∏. –°–∫–∞–∂–∏—Ç–µ —Å–µ–±–µ: —è –Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é —ç—Ç–æ. –ü–æ—Ç–æ–º —Ä–µ—à–∏—Ç–µ ‚Äî –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ç–∞–∫ –∂–∏—Ç—å.\"},\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –ú–∏—Ö–∞–∏–ª –õ–∞–±–∫–æ–≤—Å–∫–∏–π. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø—Ä—è–º–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"},\n",
    "    {\"role\": \"documents\", \"content\": json.dumps(documents, ensure_ascii=False)},\n",
    "    {\"role\": \"user\", \"content\": \"–£ –º–µ–Ω—è 4 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: —Ç—Ä–∞—Ç—ã, –∞–ª–∫–æ–≥–æ–ª—å, –∂–µ–Ω–∞—Ç—ã–π –º—É–∂—á–∏–Ω–∞, —Ç–µ–ª–µ—Ñ–æ–Ω. –° —á–µ–≥–æ –Ω–∞—á–∞—Ç—å?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
