#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Inference for Labkovsky model trained with Unsloth.
Handles RS tokens properly.

Usage:
    python inference_unsloth.py
"""

import json
import pickle
import re
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from sentence_transformers import SentenceTransformer
import chromadb

# ==============================================================
# SETTINGS
# ==============================================================

PROJECT_ROOT = Path(__file__).resolve().parent
LORA_PATH = PROJECT_ROOT / "models" / "labkovsky-qwen7b-lora-unsloth"
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"
CHROMA_DIR = PROJECT_ROOT / "chroma_db"
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
RS_CLASSIFIER_PATH = PROJECT_ROOT / "models" / "rs_classifier" / "rs_classifier.pkl"

RS_TOKENS = ["<RS=INTERVENTION>", "<RS=EXPLANATION>", "<RS=ESCALATION>"]
NUM_FEW_SHOT = 3  # Number of examples to retrieve for content

# Context instruction - forces model to USE the retrieved content
CONTEXT_INSTRUCTION = """–¢—ã ‚Äî –ú–∏—Ö–∞–∏–ª –õ–∞–±–∫–æ–≤—Å–∫–∏–π. –ù–∏–∂–µ —Ç–≤–æ–∏ –†–ï–ê–õ–¨–ù–´–ï –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã.

–í–ê–ñ–ù–û: –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∏–∂–µ. –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–π, –Ω–æ –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π –Ω–æ–≤–æ–µ.

---
–¢–í–û–ò –ü–†–ï–î–´–î–£–©–ò–ï –û–¢–í–ï–¢–´:
---"""

# Global state
_embed_model = None
_collection = None
_rs_classifier = None
_rs_id2label = None

# ==============================================================
# RAG: CHROMADB RETRIEVAL
# ==============================================================

def load_rag_corpus():
    """Load ChromaDB collection for RAG retrieval."""
    global _embed_model, _collection

    if _collection is not None:
        return

    print("üìö Loading RAG from ChromaDB...")

    # Load embedding model
    _embed_model = SentenceTransformer(EMBEDDING_MODEL)

    # Connect to ChromaDB
    client = chromadb.PersistentClient(path=str(CHROMA_DIR))
    _collection = client.get_collection("labkovsky")

    print(f"   Loaded {_collection.count()} documents")
    print("‚úÖ RAG ready")


def retrieve_similar(question: str, top_k: int = NUM_FEW_SHOT):
    """Retrieve similar documents from ChromaDB."""
    query_embedding = _embed_model.encode(f"query: {question}")

    results = _collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    retrieved = []
    for i in range(len(results['ids'][0])):
        doc = results['documents'][0][i]
        metadata = results['metadatas'][0][i]

        # Extract RS from metadata if available
        rs = metadata.get('response_signal', 'INTERVENTION')

        retrieved.append({
            'text': doc,
            'rs': rs,
            'source': metadata.get('source_file', 'unknown')
        })

    return retrieved


# ==============================================================
# RS CLASSIFIER
# ==============================================================

def load_rs_classifier():
    """Load RS classifier model."""
    global _rs_classifier, _rs_id2label

    if _rs_classifier is not None:
        return

    print("üéØ Loading RS classifier...")

    with open(RS_CLASSIFIER_PATH, "rb") as f:
        model_data = pickle.load(f)

    _rs_classifier = model_data["classifier"]
    _rs_id2label = model_data["id2rs"]

    print(f"   Labels: {list(_rs_id2label.values())}")
    print("‚úÖ RS classifier ready")


def predict_rs(question: str) -> str:
    """Predict response signal for a question."""
    question_prefixed = f"query: {question}"
    embedding = _embed_model.encode([question_prefixed])
    pred_id = _rs_classifier.predict(embedding)[0]
    return _rs_id2label[pred_id]


# ==============================================================
# LOAD MODEL
# ==============================================================

def load_model():
    print("ü§ñ Loading model...")
    
    # Quantization config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # Load tokenizer from LoRA path (has RS tokens)
    tokenizer = AutoTokenizer.from_pretrained(LORA_PATH, trust_remote_code=True)
    print(f"   Tokenizer vocab size: {len(tokenizer)}")
    
    # Resize base model embeddings to match tokenizer
    base_model.resize_token_embeddings(len(tokenizer))
    
    # Load LoRA adapter
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    model.eval()
    
    print("‚úÖ Model ready")
    return model, tokenizer


def ask(model, tokenizer, question, rs="INTERVENTION", use_rag=True, verbose=False):
    """Generate response for a question with optional RAG few-shot examples."""

    if use_rag and _collection is not None:
        # Retrieve similar examples from ChromaDB
        examples = retrieve_similar(question, top_k=NUM_FEW_SHOT)

        # Build context prompt - emphasize using retrieved content
        context_parts = []
        for i, ex in enumerate(examples, 1):
            context_parts.append(f"[{i}] {ex['text']}")

        context = "\n\n".join(context_parts)

        prompt = f"""{CONTEXT_INSTRUCTION}

{context}

---

<RS={rs}>
–í–æ–ø—Ä–æ—Å: {question}
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—ã—à–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.
–û—Ç–≤–µ—Ç:"""

        if verbose:
            print(f"\n[RAG] Using {len(examples)} examples from ChromaDB")
            for i, ex in enumerate(examples, 1):
                preview = ex['text'][:80].replace('\n', ' ')
                print(f"  [{i}] ({ex['source']}) {preview}...")
            print(f"[RAG] Total prompt tokens: ~{len(prompt.split())}")
    else:
        prompt = f"<RS={rs}>\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
        if verbose:
            print("[RAG] Disabled or not loaded")

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.4,
            do_sample=True,
            top_p=0.85,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.3,
        )

    # Decode only the generated part
    input_len = inputs["input_ids"].shape[-1]
    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)

    # Clean up - stop at various markers
    stop_markers = ["#", "–ò—Å—Ç–æ—á–Ω–∏–∫", "http", "–ß—Ç–æ –º–æ–∂–Ω–æ", "<RS=", "---", "–í–æ–ø—Ä–æ—Å:", "\n\n\n"]
    for stop in stop_markers:
        if stop in response:
            response = response.split(stop)[0]

    # Remove trailing gibberish (non-Cyrillic text at the end)
    # Find last complete Russian sentence
    sentences = re.split(r'(?<=[.!?])\s+', response)
    cleaned = []
    for sent in sentences:
        # Skip if sentence has too much non-Cyrillic
        cyrillic_ratio = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', sent)) / max(len(sent), 1)
        if cyrillic_ratio < 0.5:
            break
        cleaned.append(sent)

    return ' '.join(cleaned).strip() if cleaned else response.strip()


# ==============================================================
# MAIN
# ==============================================================

def main():
    print("=" * 60)
    print("üé§ Labkovsky Inference (Unsloth-trained + RAG + RS)")
    print("=" * 60)

    # Load RAG corpus first (also loads embedding model)
    load_rag_corpus()

    # Load RS classifier (uses same embedding model)
    load_rs_classifier()

    model, tokenizer = load_model()

    # Test questions
    test_questions = [
        "–ú—É–∂ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å —Ä–µ–±–µ–Ω–∫–æ–º, —á—Ç–æ –¥–µ–ª–∞—Ç—å?",
        "–ö–∞–∫ –ø–æ–ª—é–±–∏—Ç—å —Å–µ–±—è?",
        "–ü–∞—Ä–µ–Ω—å –Ω–µ –∑–≤–æ–Ω–∏—Ç —É–∂–µ –Ω–µ–¥–µ–ª—é, –∂–¥–∞—Ç—å –∏–ª–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –ø–µ—Ä–≤–æ–π?",
    ]
    
    print("\n" + "=" * 60)
    print("üìù Test responses (with RAG + RS classifier):")
    print("=" * 60)

    for q in test_questions:
        rs = predict_rs(q)
        print(f"\n‚ùì {q}")
        print(f"üéØ RS: {rs}")
        answer = ask(model, tokenizer, q, rs=rs, use_rag=True, verbose=True)
        print(f"üí¨ {answer}")
        print("-" * 40)
    
    # Interactive mode
    print("\n" + "=" * 60)
    print("Interactive mode (type 'exit' to quit, 'verbose' to toggle)")
    print("=" * 60)

    verbose = True

    while True:
        try:
            q = input("\n–¢—ã: ").strip()
            if q.lower() in ['exit', 'quit', 'q', '–≤—ã—Ö–æ–¥']:
                print("üëã –ü–æ–∫–∞!")
                break
            if q.lower() == 'verbose':
                verbose = not verbose
                print(f"Verbose: {'ON' if verbose else 'OFF'}")
                continue
            if not q:
                continue

            rs = predict_rs(q)
            if verbose:
                print(f"üéØ RS: {rs}")
            answer = ask(model, tokenizer, q, rs=rs, use_rag=True, verbose=verbose)
            print(f"\n–õ–∞–±–∫–æ–≤—Å–∫–∏–π [{rs}]: {answer}")

        except (KeyboardInterrupt, EOFError):
            print("\nüëã –ü–æ–∫–∞!")
            break


if __name__ == "__main__":
    main()