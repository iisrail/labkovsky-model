#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transform Labkovsky Q&A Corpus for RAG using LLM.

Uses Qwen2.5-7B-Instruct to intelligently split and condense answers
into EXPLANATION and INTERVENTION chunks.

Usage (in WSL with GPU):
    python src/data_processing/transform_for_rag_llm.py
"""

import json
import re
import torch
from pathlib import Path
from unsloth import FastLanguageModel
from tqdm import tqdm

# ============================================================
# PATHS
# ============================================================

SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent
INPUT_FILE = PROJECT_ROOT / "data" / "fine_tuning" / "qa_rs_corpus_short.jsonl"
OUTPUT_FILE = PROJECT_ROOT / "data" / "fine_tuning" / "qa_corpus_rag_optimized.jsonl"
OUTPUT_FILE_TEST = PROJECT_ROOT / "data" / "fine_tuning" / "qa_corpus_rag_test.jsonl"

# Model
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
MAX_SEQ_LENGTH = 2048

# ============================================================
# TRANSFORMATION PROMPT
# ============================================================

SYSTEM_PROMPT = """–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫–æ—Ä–ø—É—Å–∞ Q&A –ú–∏—Ö–∞–∏–ª–∞ –õ–∞–±–∫–æ–≤—Å–∫–æ–≥–æ –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã.

## –¢–≤–æ—è –∑–∞–¥–∞—á–∞
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω Q&A –≤ 1-2 —á–∞–Ω–∫–∞: EXPLANATION –∏/–∏–ª–∏ INTERVENTION.

## –ü—Ä–∞–≤–∏–ª–∞

### 1. –ê—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å (–ö–†–ò–¢–ò–ß–ù–û)
- –ù–ï –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π, –ù–ï —Å–º—è–≥—á–∞–π
- –ú–û–ñ–ù–û —Å–æ–∫—Ä–∞—â–∞—Ç—å –∏ –ø–µ—Ä–µ—Å—Ç–∞–≤–ª—è—Ç—å
- –ù–£–ñ–ù–û —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑–∫–æ—Å—Ç—å –∏ —Å—Ç–∏–ª—å –õ–∞–±–∫–æ–≤—Å–∫–æ–≥–æ

### 2. –õ–æ–≥–∏–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è

**–ï—Å—Ç—å –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –∏ –¥–∏—Ä–µ–∫—Ç–∏–≤–∞** ‚Üí –†–∞–∑–¥–µ–ª–∏ –Ω–∞ 2:
- **EXPLANATION**: –æ–±—ä—è—Å–Ω—è–µ—Ç "–ø–æ—á–µ–º—É", –ë–ï–ó –∏–º–ø–µ—Ä–∞—Ç–∏–≤–æ–≤
- **INTERVENTION**: —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–º–ø–µ—Ä–∞—Ç–∏–≤—ã –ò–õ–ò —Ä–µ—à–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∏—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã

**–¢–æ–ª—å–∫–æ –¥–∏—Ä–µ–∫—Ç–∏–≤–∞** ‚Üí INTERVENTION
**–¢–æ–ª—å–∫–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ** ‚Üí EXPLANATION

### 3. –ß—Ç–æ –£–î–ê–õ–ò–¢–¨:
- "–Ø –ø–æ–Ω–∏–º–∞—é", "–í–∞–º —Ç—è–∂–µ–ª–æ"
- "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç—É" (–∫–∞–∫ –∫–æ–Ω—Ü–æ–≤–∫–∞)
- –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã: "–ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ...?", "–ß—Ç–æ –≤—ã —á—É–≤—Å—Ç–≤—É–µ—Ç–µ?"

### 4. –ß—Ç–æ –°–û–•–†–ê–ù–ò–¢–¨:
- –ñ—ë—Å—Ç–∫–∏–µ –¥–∏–∞–≥–Ω–æ–∑—ã: "–£ –≤–∞—Å –Ω–µ–≤—Ä–æ–∑", "–≠—Ç–æ —Å–æ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å"
- "–ò–¥–∏—Ç–µ –∫ –ø—Å–∏—Ö–∏–∞—Ç—Ä—É" (–∫–∞–∫ –¥–∏—Ä–µ–∫—Ç–∏–≤–∞)
- –†–µ—à–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã: "–ê –≤–∞–º —ç—Ç–æ –Ω—É–∂–Ω–æ?", "–ó–∞—á–µ–º –≤–∞–º —ç—Ç–æ?"
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏

### 5. –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON (–±–µ–∑ markdown, –±–µ–∑ ```):
[
  {"chunk_role": "EXPLANATION", "answer": "—Ç–µ–∫—Å—Ç"},
  {"chunk_role": "INTERVENTION", "answer": "—Ç–µ–∫—Å—Ç"}
]

–ò–ª–∏ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç, –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω—É–∂–Ω–æ:
[
  {"chunk_role": "INTERVENTION", "answer": "—Ç–µ–∫—Å—Ç"}
]

## –ü—Ä–∏–º–µ—Ä

–í—Ö–æ–¥:
{"question": "–ú—É–∂ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ –¥–æ–º—É, –∫–∞–∫ –µ–≥–æ –∑–∞—Å—Ç–∞–≤–∏—Ç—å?", "answer": "–í—ã –Ω–µ –º–æ–∂–µ—Ç–µ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –≤–∑—Ä–æ—Å–ª–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞—Ç—å. –û–Ω –∂–∏–≤—ë—Ç –∫–∞–∫ —Ö–æ—á–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤—ã —ç—Ç–æ —Ç–µ—Ä–ø–∏—Ç–µ. –£ –≤–∞—Å –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: –ª–∏–±–æ –ø—Ä–∏–Ω—è—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –∏ –Ω–µ –±–µ—Å–∏—Ç—å—Å—è, –ª–∏–±–æ —É–π—Ç–∏. –í—Å—ë."}

–í—ã—Ö–æ–¥:
[
  {"chunk_role": "EXPLANATION", "answer": "–í—ã –Ω–µ –º–æ–∂–µ—Ç–µ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –≤–∑—Ä–æ—Å–ª–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞—Ç—å. –û–Ω –∂–∏–≤—ë—Ç –∫–∞–∫ —Ö–æ—á–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤—ã —ç—Ç–æ —Ç–µ—Ä–ø–∏—Ç–µ."},
  {"chunk_role": "INTERVENTION", "answer": "–£ –≤–∞—Å –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: –ª–∏–±–æ –ø—Ä–∏–Ω—è—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –∏ –Ω–µ –±–µ—Å–∏—Ç—å—Å—è, –ª–∏–±–æ —É–π—Ç–∏. –í—Å—ë."}
]"""

USER_PROMPT_TEMPLATE = """–ü—Ä–µ–æ–±—Ä–∞–∑—É–π —ç—Ç–æ—Ç Q&A:

{{"question": "{question}", "answer": "{answer}"}}

–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ —Å 1-2 —á–∞–Ω–∫–∞–º–∏."""

# ============================================================
# MODEL LOADING
# ============================================================

def load_model():
    """Load Qwen model with unsloth."""
    print(f"ü§ñ Loading model: {MODEL_NAME}")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,  # Auto-detect
        load_in_4bit=True,
    )

    # Enable faster inference
    FastLanguageModel.for_inference(model)

    print("‚úÖ Model loaded")
    return model, tokenizer


# ============================================================
# TRANSFORMATION
# ============================================================

def transform_record(model, tokenizer, record: dict) -> list[dict]:
    """Transform a single record using LLM."""

    question = record['question']
    answer = record['answer']

    user_prompt = USER_PROMPT_TEMPLATE.format(
        question=question.replace('"', '\\"'),
        answer=answer.replace('"', '\\"')
    )

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]

    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to(model.device)

    with torch.inference_mode():
        outputs = model.generate(
            inputs,
            max_new_tokens=1024,
            temperature=0.3,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )

    input_len = inputs.shape[-1]
    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()

    # Parse JSON from response
    chunks = parse_llm_response(response, record)

    return chunks


def parse_llm_response(response: str, record: dict) -> list[dict]:
    """Parse LLM response and create output chunks."""

    # Try to extract JSON array
    try:
        # Find JSON array in response
        match = re.search(r'\[[\s\S]*\]', response)
        if match:
            json_str = match.group()
            parsed = json.loads(json_str)
        else:
            raise ValueError("No JSON array found")
    except (json.JSONDecodeError, ValueError) as e:
        print(f"  ‚ö† JSON parse error: {e}")
        print(f"    Response: {response[:200]}...")
        # Fallback: return original as EXPLANATION
        return [{
            "type": "qa",
            "chunk_role": "EXPLANATION",
            "question": record['question'],
            "answer": record['answer'],
            "id": f"{record.get('id', 'unknown')}_expl",
            "video_id": record.get('video_id', ''),
            "char_count": len(record['answer']),
            "parse_error": True
        }]

    # Build output chunks
    chunks = []
    base_id = record.get('id', 'unknown')
    video_id = record.get('video_id', '')

    for item in parsed:
        role = item.get('chunk_role', 'EXPLANATION')
        answer_text = item.get('answer', '')

        if not answer_text:
            continue

        suffix = "_expl" if role == "EXPLANATION" else "_interv"

        chunk = {
            "type": "qa",
            "chunk_role": role,
            "question": record['question'],
            "answer": answer_text,
            "id": f"{base_id}{suffix}",
            "char_count": len(answer_text)
        }

        if video_id:
            chunk["video_id"] = video_id

        chunks.append(chunk)

    return chunks if chunks else [{
        "type": "qa",
        "chunk_role": "EXPLANATION",
        "question": record['question'],
        "answer": record['answer'],
        "id": f"{base_id}_expl",
        "video_id": video_id,
        "char_count": len(record['answer']),
        "fallback": True
    }]


# ============================================================
# MAIN
# ============================================================

def process_file():
    """Main processing function."""
    print("=" * 60)
    print("Transform Labkovsky Q&A for RAG (LLM-based)")
    print("=" * 60)
    print(f"\nInput:  {INPUT_FILE}")
    print(f"Output: {OUTPUT_FILE}\n")

    # Load model
    model, tokenizer = load_model()

    # Read input
    records = []
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"  ‚ö† JSON error: {e}")

    # Limit for testing (set to None for full processing)
    TEST_LIMIT = 20
    if TEST_LIMIT:
        records = records[:TEST_LIMIT]
        output_file = OUTPUT_FILE_TEST
        print(f"\nüìÑ Testing on first {len(records)} records")
        print(f"Output: {output_file}\n")
    else:
        output_file = OUTPUT_FILE
        print(f"\nüìÑ Loaded {len(records)} input records")
        print(f"‚è± Estimated time: {len(records) * 15 // 60} minutes\n")

    # Process with progress bar
    all_chunks = []
    stats = {
        "split_both": 0,
        "single": 0,
        "parse_errors": 0,
        "expl_chars": [],
        "interv_chars": [],
    }

    # Open output file for incremental writing
    with open(output_file, 'w', encoding='utf-8') as f_out:
        for record in tqdm(records, desc="Transforming"):
            try:
                chunks = transform_record(model, tokenizer, record)

                # Track stats
                if len(chunks) == 2:
                    stats["split_both"] += 1
                else:
                    stats["single"] += 1

                for chunk in chunks:
                    if chunk.get('parse_error'):
                        stats["parse_errors"] += 1

                    if chunk['chunk_role'] == "EXPLANATION":
                        stats["expl_chars"].append(chunk['char_count'])
                    else:
                        stats["interv_chars"].append(chunk['char_count'])

                    # Write immediately
                    f_out.write(json.dumps(chunk, ensure_ascii=False) + '\n')
                    all_chunks.append(chunk)

            except Exception as e:
                print(f"\n‚ùå Error processing {record.get('id', 'unknown')}: {e}")
                # Write original as fallback
                fallback = {
                    "type": "qa",
                    "chunk_role": "EXPLANATION",
                    "question": record['question'],
                    "answer": record['answer'],
                    "id": f"{record.get('id', 'unknown')}_expl",
                    "char_count": len(record['answer']),
                    "error": str(e)
                }
                f_out.write(json.dumps(fallback, ensure_ascii=False) + '\n')
                all_chunks.append(fallback)

    # Report
    print("\n" + "=" * 60)
    print("Transformation Complete")
    print("=" * 60)
    print(f"\nInput:  {len(records)} records")
    print(f"Output: {len(all_chunks)} chunks")

    expl_count = len(stats["expl_chars"])
    interv_count = len(stats["interv_chars"])
    print(f"        ({expl_count} EXPLANATION + {interv_count} INTERVENTION)")

    print(f"\nSplit distribution:")
    print(f"  Split into 2: {stats['split_both']}")
    print(f"  Single chunk: {stats['single']}")
    print(f"  Parse errors: {stats['parse_errors']}")

    if stats["expl_chars"]:
        print(f"\nEXPLANATION char counts:")
        print(f"  avg: {sum(stats['expl_chars']) // len(stats['expl_chars'])}")
        print(f"  min: {min(stats['expl_chars'])}")
        print(f"  max: {max(stats['expl_chars'])}")

    if stats["interv_chars"]:
        print(f"\nINTERVENTION char counts:")
        print(f"  avg: {sum(stats['interv_chars']) // len(stats['interv_chars'])}")
        print(f"  min: {min(stats['interv_chars'])}")
        print(f"  max: {max(stats['interv_chars'])}")

    print(f"\n‚úÖ Output saved to: {output_file}")


if __name__ == "__main__":
    process_file()
