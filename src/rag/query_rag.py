#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Pipeline - Query (with Fine-tuned Model)
–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ fine-tuned Qwen2.5 + LoRA

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
1. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ build_index.py
2. python query_rag.py

–ò–ª–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π –∫–∞–∫ –º–æ–¥—É–ª—å:
    from query_rag import ask_labkovsky
    answer = ask_labkovsky("–ö–∞–∫ –ø–æ–ª—é–±–∏—Ç—å —Å–µ–±—è?")
"""

import argparse
import torch
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import chromadb
from config import CHROMA_DIR, MODELS_DIR

# ============================================================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================

# Embedding –º–æ–¥–µ–ª—å (—Ç–∞ –∂–µ —á—Ç–æ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!)
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# Fine-tuned LLM
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
LORA_PATH = MODELS_DIR / "labkovsky-qwen7b-lora"

# –°–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –∏—Å–∫–∞—Ç—å
TOP_K = 5

# Prompt modes: "full", "minimal", "none"
PROMPT_MODE = "full"

SYSTEM_PROMPTS = {
    "full": """–¢—ã ‚Äî –ú–∏—Ö–∞–∏–ª –õ–∞–±–∫–æ–≤—Å–∫–∏–π, –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø—Å–∏—Ö–æ–ª–æ–≥.

–¢–≤–æ–π —Å—Ç–∏–ª—å:
- –ü—Ä—è–º–æ–π, –±–µ–∑ –≤–æ–¥—ã
- –° —é–º–æ—Ä–æ–º –∏ –∏—Ä–æ–Ω–∏–µ–π
- –ò–Ω–æ–≥–¥–∞ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π
- –ì–æ–≤–æ—Ä–∏—à—å –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
- –ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∂–∏–∑–Ω–∏
- –¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è –∏–¥–µ—è: –ø—Å–∏—Ö–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑-–∑–∞ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π, –∏ –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –Ω–∏—Ö –º–æ–∂–Ω–æ, —Å–∏—Å—Ç–µ–º–Ω–æ –ø—Ä–∏–º–µ–Ω—è—è 6 –ø—Ä–∞–≤–∏–ª.

–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ.""",

    "minimal": """–¢—ã ‚Äî –ú–∏—Ö–∞–∏–ª –õ–∞–±–∫–æ–≤—Å–∫–∏–π, –ø—Å–∏—Ö–æ–ª–æ–≥. –û—Ç–≤–µ—á–∞–π –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ.""",

    "none": None
}

# ============================================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
_embed_model = None
_collection = None
_llm = None
_tokenizer = None


def init_retrieval():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding –º–æ–¥–µ–ª–∏ –∏ ChromaDB"""
    global _embed_model, _collection
    
    if _embed_model is None:
        print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏...")
        _embed_model = SentenceTransformer(EMBEDDING_MODEL)
    
    if _collection is None:
        print("üíæ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB...")
        client = chromadb.PersistentClient(path=str(CHROMA_DIR))
        _collection = client.get_collection("labkovsky")
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {_collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    return _embed_model, _collection


def init_llm(use_lora: bool = True):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fine-tuned LLM —Å LoRA"""
    global _llm, _tokenizer
    
    if _llm is None:
        print(f"ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ LLM: {MODEL_NAME}")

        
        # 4-bit quantization for 12GB VRAM
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        if use_lora:
            print(f"   LoRA: {LORA_PATH}")
            _llm = PeftModel.from_pretrained(base_model, str(LORA_PATH))
        else:
            print("   (base model, no LoRA)")
            _llm = base_model
        # Load LoRA adapter
        
        _llm.eval()
        
        # Load tokenizer
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        
        print("‚úÖ LLM –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    return _llm, _tokenizer


def init(use_lora: bool = True):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    init_retrieval()
    init_llm(use_lora)


# ============================================================
# RETRIEVAL
# ============================================================

def retrieve(query: str, top_k: int = TOP_K) -> list:
    """
    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    Args:
        query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        top_k: –°–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
    
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    embed_model, collection = init_retrieval()
    
    # –î–ª—è e5 –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω –ø—Ä–µ—Ñ–∏–∫—Å "query:"
    query_embedding = embed_model.encode(f"query: {query}")
    
    # –ü–æ–∏—Å–∫ –≤ ChromaDB
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    documents = []
    for i in range(len(results['ids'][0])):
        documents.append({
            "text": results['documents'][0][i],
            "metadata": results['metadatas'][0][i],
            "distance": results['distances'][0][i]
        })
    
    return documents


# ============================================================
# GENERATION
# ============================================================

def generate(query: str, context_docs: list, prompt_mode: str = None) -> str:
    if prompt_mode is None:
        prompt_mode = PROMPT_MODE

    llm, tokenizer = init_llm()

    # ---- –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç ----
    context_parts = []
    for i, doc in enumerate(context_docs, 1):
        source = doc['metadata'].get('source', 'unknown')
        if source == 'qa':
            context_parts.append(f"[{i}] {doc['text']}")
        else:
            identifier = (
                doc['metadata'].get('article_id')
                or doc['metadata'].get('interview_id')
                or doc['metadata'].get('book_id')
                or doc['metadata'].get('chapter_id')
                or ''
            )
            context_parts.append(f"[{i}] ({identifier}) {doc['text']}")

    context = "\n\n".join(context_parts)

    user_message = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–æ–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤:

{context}

---

–í–æ–ø—Ä–æ—Å: {query}"""

    system_prompt = SYSTEM_PROMPTS.get(prompt_mode)

    if system_prompt:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]
    else:
        messages = [{"role": "user", "content": user_message}]

    # ---- –í–ê–ñ–ù–û: tokenize=True ----
    inputs = tokenizer.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to(llm.device)

    input_len = inputs.shape[-1]

    with torch.inference_mode():
        outputs = llm.generate(
            inputs,
            max_new_tokens=1536,
            temperature=None,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    # ---- –ö–û–†–†–ï–ö–¢–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ ----
    generated_tokens = outputs[0][input_len:]
    response = tokenizer.decode(
        generated_tokens,
        skip_special_tokens=True
    ).strip()

    return response



# ============================================================
# MAIN FUNCTION
# ============================================================

def ask_labkovsky(query: str, top_k: int = TOP_K, verbose: bool = False, prompt_mode: str = None) -> str:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å ‚Äî –ø–æ–ª—É—á–∏ –æ—Ç–≤–µ—Ç –æ—Ç –õ–∞–±–∫–æ–≤—Å–∫–æ–≥–æ
    
    Args:
        query: –í–æ–ø—Ä–æ—Å
        top_k: –°–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        verbose: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        prompt_mode: "full", "minimal", or "none"
    
    Returns:
        –û—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ –õ–∞–±–∫–æ–≤—Å–∫–æ–≥–æ
    """
    # Retrieval
    docs = retrieve(query, top_k)
    
    if verbose:
        print("\nüìö –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
        for i, doc in enumerate(docs, 1):
            meta = doc['metadata']
            source = meta.get('source', 'unknown')
            dist = doc['distance']

            # Build display ID based on source
            if source == 'qa':
                display_id = meta.get('id', '?')
            elif source == 'article':
                display_id = f"{meta.get('article_id', '?')[:20]}..._chunk{meta.get('chunk_id')}"
            elif source == 'interview':
                display_id = f"int_chunk{meta.get('chunk_id')}"
            elif source == 'book':
                display_id = f"{meta.get('book_id', '?')}_{meta.get('chapter_id', '?')}"
            else:
                display_id = '?'
            
            preview = doc['text'][:80] + "..."
            print(f"  [{i}] ({source}, {display_id}, dist={dist:.3f}) {preview}")
        print()
    
    # Generation
    answer = generate(query, docs, prompt_mode)
    
    return answer


# ============================================================
# CLI
# ============================================================

def main():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º"""
    global PROMPT_MODE

        # Parse command line args
    parser = argparse.ArgumentParser()
    parser.add_argument('--no-lora', action='store_true', help='Use base model without LoRA')
    args = parser.parse_args()
    use_lora = not args.no_lora
    
    print("="*60)
    print("üé§ –°–ø—Ä–æ—Å–∏ –õ–∞–±–∫–æ–≤—Å–∫–æ–≥–æ! (Fine-tuned)")
    print("="*60)
    print("–ö–æ–º–∞–Ω–¥—ã:")
    print("  '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'exit' - –≤—ã—Ö–æ–¥")
    print("  'verbose' - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
    print("  'full' / 'minimal' / 'none' - —Ä–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞")
    print()
    
    verbose = False
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    init(use_lora)
    print(f"\n–†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞: {PROMPT_MODE}")
    print()
    
    while True:
        try:
            query = input("–¢—ã: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nüëã –ü–æ–∫–∞!")
            break
        
        if not query:
            continue
        
        if query.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
            print("üëã –ü–æ–∫–∞!")
            break
        
        if query.lower() == 'verbose':
            verbose = not verbose
            print(f"Verbose —Ä–µ–∂–∏–º: {'–≤–∫–ª—é—á–µ–Ω' if verbose else '–≤—ã–∫–ª—é—á–µ–Ω'}")
            continue
        
        if query.lower() in ['full', 'minimal', 'none']:
            PROMPT_MODE = query.lower()
            print(f"–†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞: {PROMPT_MODE}")
            continue
        
        print("\nü§î –î—É–º–∞—é...\n")
        
        try:
            answer = ask_labkovsky(query, verbose=verbose)
            print(f"–õ–∞–±–∫–æ–≤—Å–∫–∏–π: {answer}\n")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}\n")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()