#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Pipeline - Build Index
–°–æ–∑–¥–∞—ë—Ç embeddings –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
1. pip install chromadb sentence-transformers
2. python build_index.py

–í—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–∞–ø–∫–∞ data/):
- articles_with_questions.jsonl
- interviews.jsonl  
- qa_pairs.jsonl

–í—ã—Ö–æ–¥:
- ./chroma_db/ (vector store)
"""

import json
import os
from pathlib import Path
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# ============================================================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================

SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR.parent / "data" / "processed"
CHROMA_DIR = SCRIPT_DIR.parent / "chroma_db"

# Embedding –º–æ–¥–µ–ª—å (–æ—Ç–ª–∏—á–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# –§–∞–π–ª—ã –¥–ª—è RAG
RAG_FILES = [
    "articles_with_questions.jsonl",
    "interviews.jsonl",
    "qa_pairs.jsonl"
]

# ============================================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================

def load_jsonl(filepath):
    """–ó–∞–≥—Ä—É–∑–∫–∞ JSONL —Ñ–∞–π–ª–∞"""
    records = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                records.append(json.loads(line))
    return records


def prepare_documents():
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: texts, metadatas, ids
    """
    texts = []
    metadatas = []
    ids = []
    
    doc_id = 0
    
    for filename in RAG_FILES:
        filepath = DATA_DIR / filename
        
        if not filepath.exists():
            print(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            continue
        
        records = load_jsonl(filepath)
        print(f"üìÑ {filename}: {len(records)} –∑–∞–ø–∏—Å–µ–π")
        
        for record in records:
            source = record.get("source", "unknown")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embedding
            if source == "article":
                # –°—Ç–∞—Ç—å–∏: text + potential_questions
                text = record["text"]
                questions = record.get("potential_questions", [])
                if questions:
                    text = " ".join(questions) + " " + text
                
                metadata = {
                    "source": "article",
                    "article_id": record.get("article_id", ""),
                    "chunk_id": record.get("chunk_id", 0)
                }
                
            elif source == "interview":
                # –ò–Ω—Ç–µ—Ä–≤—å—é: text + potential_questions
                text = record["text"]
                questions = record.get("potential_questions", [])
                if questions:
                    text = " ".join(questions) + " " + text
                
                metadata = {
                    "source": "interview",
                    "interview_id": record.get("interview_id", ""),
                    "topic": record.get("topic", ""),
                    "chunk_id": record.get("chunk_id", 0)
                }
                
            else:
                # Q&A pairs: question + answer
                question = record.get("question", "")
                answer = record.get("answer", "")
                text = f"–í–æ–ø—Ä–æ—Å: {question} –û—Ç–≤–µ—Ç: {answer}"
                
                metadata = {
                    "source": "qa",
                    "video_id": record.get("video_id", ""),
                    "video_title": record.get("video_title", ""),
                    "question": question
                }
            
            # –î–ª—è e5 –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω –ø—Ä–µ—Ñ–∏–∫—Å "passage:"
            text_for_embedding = f"passage: {text}"
            
            texts.append(text_for_embedding)
            metadatas.append(metadata)
            ids.append(f"doc_{doc_id}")
            doc_id += 1
    
    return texts, metadatas, ids


# ============================================================
# EMBEDDING + VECTOR STORE
# ============================================================

def build_index():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞"""
    
    print("="*60)
    print("RAG Pipeline - Build Index")
    print("="*60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    texts, metadatas, ids = prepare_documents()
    print(f"‚úÖ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")
    
    if not texts:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")
        return
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏
    print(f"\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {EMBEDDING_MODEL}...")
    print("   (–ø–µ—Ä–≤—ã–π —Ä–∞–∑ —Å–∫–∞—á–∞–µ—Ç ~2GB)")
    model = SentenceTransformer(EMBEDDING_MODEL)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ embeddings
    print(f"\nüî¢ –°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    embeddings = model.encode(texts, show_progress_bar=True)
    print(f"‚úÖ Embeddings —Å–æ–∑–¥–∞–Ω—ã: shape {embeddings.shape}")
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ ChromaDB
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ ChromaDB ({CHROMA_DIR})...")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –±–∞–∑—É –µ—Å–ª–∏ –µ—Å—Ç—å
    if CHROMA_DIR.exists():
        import shutil
        shutil.rmtree(CHROMA_DIR)
    
    # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç
    client = chromadb.PersistentClient(path=str(CHROMA_DIR))
    
    # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection = client.create_collection(
        name="labkovsky",
        metadata={"description": "Labkovsky RAG knowledge base"}
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    collection.add(
        ids=ids,
        embeddings=embeddings.tolist(),
        metadatas=metadatas,
        documents=[t.replace("passage: ", "") for t in texts]  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞
    )
    
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*60)
    print("‚úÖ –ò–ù–î–ï–ö–° –°–û–ó–î–ê–ù!")
    print("="*60)
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts)}")
    print(f"   Embedding —Ä–∞–∑–º–µ—Ä: {embeddings.shape[1]}")
    print(f"   –•—Ä–∞–Ω–∏–ª–∏—â–µ: {CHROMA_DIR}")
    print("\n   –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏: python query_rag.py")


if __name__ == "__main__":
    build_index()
